<!DOCTYPE html>
<html lang="zh-Hans">

<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="x5-fullscreen" content="true">
<meta name="full-screen" content="yes">
<meta name="theme-color" content="#317EFB" />
<meta content="width=device-width, initial-scale=1.0, maximum-scale=5.0, user-scalable=0" name="viewport">
<meta name="description" content="深度学习记录 吴恩达课程学习记录吴恩达 - 机器学习笔记_吴恩达机器学习最新版笔记-CSDN博客 吴恩达机器学习笔记-CSDN博客 kaieye&#x2F;2022-Machine-Learning-Specialization (github.com) Machine Learning: Regression and Classificationweek1机器学习机器学习的核心在于通过大量数据的学习，让机">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习">
<meta property="og:url" content="http://example.com/2024/08/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="Blog">
<meta property="og:description" content="深度学习记录 吴恩达课程学习记录吴恩达 - 机器学习笔记_吴恩达机器学习最新版笔记-CSDN博客 吴恩达机器学习笔记-CSDN博客 kaieye&#x2F;2022-Machine-Learning-Specialization (github.com) Machine Learning: Regression and Classificationweek1机器学习机器学习的核心在于通过大量数据的学习，让机">
<meta property="og:locale">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="og:image" content="http://example.com/img/404.jpg">
<meta property="article:published_time" content="2024-08-19T12:24:46.476Z">
<meta property="article:modified_time" content="2024-10-04T12:20:14.310Z">
<meta property="article:author" content="mengla1">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/404.jpg">


<title >深度学习</title>

<!-- Favicon -->

    <link href='/img/favicon.svg?v=2.2.2' rel='icon' type='image/png' sizes='16x16' ></link>


    <link href='/img/favicon.svg?v=2.2.2' rel='icon' type='image/png' sizes='32x32' ></link>




<!-- Plugin -->




    
<link rel="stylesheet" href="/css/plugins/bootstrap.row.css">

    
<link rel="stylesheet" href="https://unpkg.com/@fancyapps/ui@4.0/dist/fancybox.css">

    
    
<link rel="stylesheet" href="https://unpkg.com/katex@latest/dist/katex.min.css">





<!-- Icon -->

    
<link rel="stylesheet" href="/css/plugins/font-awesome.min.css">




<!-- Variable -->
<script>window.ASYNC_CONFIG = {"hostname":"example.com","author":"mengla1","root":"/","typed_text":null,"theme_version":"2.2.2","theme":{"switch":true,"default":"style-light"},"favicon":{"logo":"/img/favicon.svg","icon16":"/img/favicon.svg","icon32":"/img/favicon.svg","apple_touch_icon":null,"webmanifest":null,"visibilitychange":false,"hidden":"/failure.ico","show_text":"Go","hide_text":"Left"},"i18n":{"placeholder":"搜索文章...","empty":"找不到您查询的内容: ${query}","hits":"找到 ${hits} 条结果","hits_time":"找到 ${hits} 条结果（用时 ${time} 毫秒）","author":"本文作者：","copyright_link":"本文链接：","copyright_license_title":"版权声明：","copyright_license_content":"本博客所有文章除特别声明外，均默认采用 undefined 许可协议。","copy_success":"复制成功","copy_failure":"复制失败","open_read_mode":"进入阅读模式","exit_read_mode":"退出阅读模式","notice_outdate_message":"距离上次更新已经 undefined 天了, 文章内容可能已经过时。","sticky":"置顶","just":"刚刚","min":"分钟前","hour":"小时前","day":"天前","month":"个月前"},"swup":false,"plugin":{"flickr_justified_gallery":"https://unpkg.com/flickr-justified-gallery@latest/dist/fjGallery.min.js"},"icons":{"sun":"far fa-sun","moon":"far fa-moon","play":"fas fa-play","email":"far fa-envelope","next":"fas fa-arrow-right","calendar":"far fa-calendar-alt","clock":"far fa-clock","user":"far fa-user","back_top":"fas fa-arrow-up","close":"fas fa-times","search":"fas fa-search","reward":"fas fa-hand-holding-usd","user_tag":"fas fa-user-alt","toc_tag":"fas fa-th-list","read":"fas fa-book-reader","arrows":"fas fa-arrows-alt-h","double_arrows":"fas fa-angle-double-down","copy":"fas fa-copy"},"icontype":"font","highlight":{"plugin":"highlighjs","theme":true,"copy":true,"lang":true,"title":"default","height_limit":false},"toc":{"post_title":true},"live_time":{"start_time":"","prefix":"博客已萌萌哒运行 undefined 天"},"danmu":{"enable":false,"el":".trm-banner"}};</script>
<script id="async-page-config">window.PAGE_CONFIG = {"isPost":true,"isHome":false,"postUpdate":"2024-10-04 20:20:14"};</script>

<!-- Theme mode css -->
<link data-swup-theme rel="stylesheet" href="/css/index.css?v=2.2.2" id="trm-switch-style">
<script>
    let defaultMode = ASYNC_CONFIG.theme.default !=='auto' ?  ASYNC_CONFIG.theme.default : (window.matchMedia("(prefers-color-scheme: light)").matches ? 'style-light' : 'style-dark')
    let catchMode = localStorage.getItem('theme-mode') || defaultMode;
    let type = catchMode === 'style-dark' ? 'add' : 'remove';
    document.documentElement.classList[type]('dark')
</script>

<!-- CDN -->


    
    



<!-- Site Analytics -->

 
<meta name="generator" content="Hexo 7.3.0"></head>

<body>

  <!-- app wrapper -->
  <div class="trm-app-frame">

    <!-- page preloader -->
    <div class="trm-preloader">
    <div class="trm-holder">
        <div class="preloader">
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
            <div></div>
        </div>
    </div>
</div>
    <!-- page preloader end -->

    <!-- change mode preloader -->
    <div class="trm-mode-swich-animation-frame">
    <div class="trm-mode-swich-animation">
        <i class="i-sun"><i class="iconfont far fa-sun"></i></i>
        <div class="trm-horizon"></div>
        <i class="i-moon"><i class="iconfont far fa-moon"></i></i>
    </div>
</div>
    <!-- change mode preloader end -->

      <!-- scroll container -->
      <div id="trm-dynamic-content" class="trm-swup-animation">
        <div id="trm-scroll-container" class="trm-scroll-container" style="opacity: 0">
            <!-- top bar -->
            <header class="trm-top-bar">
	<div class="container">
		<div class="trm-left-side">
			<!-- logo -->
<a href="/" class="trm-logo-frame trm-anima-link">
    
        <img alt="logo" src="/img/favicon.svg">
    
    
        <div class="trm-logo-text">
            Async<span>Theme</span>
        </div>
    
</a>
<!-- logo end -->
		</div>
		<div class="trm-right-side">
			<!-- menu -->
<div class="trm-menu">
    <nav>
        <ul>
            
            <li class="menu-item-has-children ">
                <a  href="/" target="">
                    首页
                </a>
                
            </li>
            
            <li class="menu-item-has-children ">
                <a  href="/archives/" target="">
                    归档
                </a>
                
            </li>
            
        </ul>
    </nav>
</div>
<!-- menu end -->
			
    <!-- mode switcher place -->
    <div class="trm-mode-switcher-place">
        <div class="trm-mode-switcher">
            <i class="iconfont far fa-sun"></i>
            <input class="tgl tgl-light" id="trm-swich" type="checkbox">
            <label class="trm-swich" for="trm-swich"></label>
            <i class="iconfont far fa-moon"></i>
        </div>
    </div>
    <!-- mode switcher place end -->

			
		</div>
		<div class="trm-menu-btn">
			<span></span>
		</div>
	</div>
</header>
            <!-- top bar end -->

            <!-- body -->
            
<div class="trm-content-start">
    <!-- banner -->
    <div class="trm-banner">
    
    <!-- banner cover -->
    <img style="object-position:top;object-fit:cover;" alt="banner" class="trm-banner-cover" src="/img/banner.png">
    <!-- banner cover end -->
    

    <!-- banner content -->
    <div class="trm-banner-content trm-overlay">
        <div class="container">
            <div class="row">
                
                <div class="col-lg-12">

                    <!-- banner title -->
                    <div class="trm-banner-text trm-text-center ">
                        <div class="trm-label trm-mb-20">
                            NEWS LETTER
                        </div>
                        <h1 class="trm-mb-30 trm-hsmb-font">
                            深度学习
                        </h1>

                        
                            <ul class="trm-breadcrumbs trm-label">
                                <li>
                                    <a href="/" class="trm-anima-link">Home</a>
                                </li>
                                <li>
                                    <span>
                                        2024
                                    </span>
                                </li>
                            </ul>
                        
                    </div>
                    <!-- banner title end -->

                    <!-- scroll hint -->
                    <span id="scroll-triger" class="trm-scroll-hint-frame">
                        <div class="trm-scroll-hint"></div>
                        <span class="trm-label">Scroll down</span>
                    </span>
                    <!-- scroll hint end -->

                </div>
            </div>
        </div>
    </div>
    <!-- banner content end -->
</div>
    <!-- banner end -->
    <div class="container">
        <div class="row">
            
            <div class="trm-page-content col-lg-12">
                <div id="trm-content" class="trm-content">
                    <div class="trm-post-info row hidden-sm">
    <div class="col-sm-4">
        <div class="trm-card trm-label trm-label-light text-center">
            <i class="iconfont far fa-calendar-alt trm-icon"></i><br>
            08/19
        </div>
    </div>
    <div class="col-sm-4">
        <div class="trm-card trm-label trm-label-light text-center">
            <i class="iconfont far fa-clock trm-icon"></i><br>
            20:24
        </div>
    </div>
    <div class="col-sm-4">
        <div id="post-author" class="trm-card trm-label trm-label-light text-center">
            <i class="iconfont far fa-user trm-icon"></i><br>
            mengla1
        </div>
    </div>
</div>
<div class="trm-card ">
    <article id="article-container" class="trm-publication">
    <p>深度学习记录</p>
<h1 id="吴恩达课程学习记录"><a href="#吴恩达课程学习记录" class="headerlink" title="吴恩达课程学习记录"></a>吴恩达课程学习记录</h1><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_43306527/article/details/138049893">吴恩达 - 机器学习笔记_吴恩达机器学习最新版笔记-CSDN博客</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_52649952/article/details/140923440">吴恩达机器学习笔记-CSDN博客</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/kaieye/2022-Machine-Learning-Specialization/tree/main">kaieye/2022-Machine-Learning-Specialization (github.com)</a></p>
<h2 id="Machine-Learning-Regression-and-Classification"><a href="#Machine-Learning-Regression-and-Classification" class="headerlink" title="Machine Learning: Regression and Classification"></a>Machine Learning: Regression and Classification</h2><h3 id="week1"><a href="#week1" class="headerlink" title="week1"></a>week1</h3><h4 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a><strong>机器学习</strong></h4><p>机器学习的核心在于通过大量数据的学习，让机器逐步理解并掌握现实中的规律，进而构建出更符合实际情况的模型。通过应用这些模型，机器可以在各种任务中展现出比以往更加优异的表现。简而言之，机器学习通过给定的训练数据集学习一个函数（即模型参数），使得当新数据出现时，模型能够基于这个函数对结果进行预测。</p>
<h4 id="监督学习（Supervised-Learning）"><a href="#监督学习（Supervised-Learning）" class="headerlink" title="监督学习（Supervised Learning）"></a><strong>监督学习（Supervised Learning）</strong></h4><p>在监督学习中，==训练集包含成对的输入和输出（即“正确答案”），也可以理解为特征和目标==。例如，输入（x）是一封电子邮件，输出则是判断该邮件是否为垃圾邮件。这个过程最终会生成一个垃圾邮件过滤器。再比如，输入是一段语音，输出是相应的文本，这对应的是语音识别任务。或者输入是语言A，输出是语言B，这就是机器翻译任务。简单来说，监督学习的本质是将样本集提供给机器学习算法，让算法从样本中学习，之后当接收到新的输入（即未在样本集中出现的数据）时，算法能够进行正确的预测和输出（y），并且通过持续训练，不断优化结果。</p>
<p>==监督学习分为两大类任务：回归和分类。它们的主要区别在于预测目标的类型。==</p>
<p><strong>分类（Classification）：</strong></p>
<p>分类任务的目标是将输入数据划分为预定义的类别或标签。常见的分类类型包括：</p>
<ul>
<li><strong>二分类（Binary Classification）</strong>：输出只有两个可能的类别。例如，肿瘤是否为恶性（是/否）、电子邮件是否为垃圾邮件。</li>
<li><strong>多分类（Multi-class Classification）</strong>：输出可以是多个类别之一。例如，图像识别中预测输入图片是猫、狗或鸟。</li>
</ul>
<p>分类的关键特征是输出为离散的类别标签，模型通过学习数据与这些类别之间的关系来进行预测。</p>
<p><strong>回归（Regression）：</strong></p>
<p>回归任务的目标是预测一个连续的数值输出。在这种情况下，模型需要预测的是一个实数值，而不是离散的类别标签。常见的回归任务包括：</p>
<ul>
<li><strong>线性回归（Linear Regression）</strong>：预测变量与一个或多个输入变量呈线性关系。例如，根据房屋的面积和位置预测房价。</li>
<li><strong>多项式回归（Polynomial Regression）</strong>：输入变量和预测变量之间是非线性的，通过拟合更高阶的多项式函数来描述这种关系。</li>
</ul>
<p>回归的关键特征是输出为连续的数值，模型通过学习输入数据与数值之间的映射关系进行预测。</p>
<p><strong>区别总结</strong></p>
<ul>
<li><strong>分类</strong>：输出是离散的类别标签（如A、B、C）。</li>
<li><strong>回归</strong>：输出是连续的数值（如温度、价格）。</li>
</ul>
<h4 id="无监督学习（Unsupervised-Learning）"><a href="#无监督学习（Unsupervised-Learning）" class="headerlink" title="无监督学习（Unsupervised Learning）"></a><strong>无监督学习（Unsupervised Learning）</strong></h4><p>无监督学习是一种机器学习方法，其特点是不依赖于预先标注好的训练数据集。在无监督学习中，数据集仅包含输入数据（没有对应的输出或标签），==算法需要从数据中自行发现结构、模式或规律==。无监督学习通常应用于探索性数据分析、数据降维以及特征提取等任务。以下是几种常见的无监督学习任务：</p>
<ol>
<li><strong>聚类（Clustering）</strong></li>
</ol>
<p>聚类是无监督学习中最典型的任务之一，它用于将数据集中的样本自动分组。聚类算法不依赖预先标注的标签，而是根据数据本身的特征，试图找到相似样本的自然分布。例如，在客户分类、市场细分、图像分割等领域，聚类都起着至关重要的作用。</p>
<ol>
<li><strong>异常检测（Anomaly Detection）</strong></li>
</ol>
<p>异常检测的目的是识别那些与数据集中的大部分样本不同的“异常”数据点。由于异常事件通常是稀有的且缺乏标签，使用无监督学习来检测异常情况是非常有效的。这在网络安全（如检测入侵行为）、工业设备故障监控、金融欺诈检测等领域尤为常见。</p>
<ol>
<li><strong>降维（Dimensionality Reduction）</strong></li>
</ol>
<p>降维也是无监督学习的重要任务之一，主要用于处理高维数据。当数据维度过高时，分析和建模会变得困难，且容易出现“维度灾难”。通过降维技术，可以保留数据的主要特征，同时减少计算复杂度和噪声。这在数据可视化、特征提取和加速机器学习模型训练中都非常有用。</p>
<p><strong>无监督学习的特点</strong></p>
<ul>
<li><strong>没有标签</strong>：与监督学习不同，无监督学习的数据没有对应的正确答案，算法需要在没有明确指导的情况下自行探索和学习。</li>
<li><strong>模式发现</strong>：无监督学习的目标往往是发现数据中的潜在模式或结构，而不是生成具体的预测结果。</li>
</ul>
<h4 id="线性回归模型"><a href="#线性回归模型" class="headerlink" title="线性回归模型"></a><strong>线性回归模型</strong></h4><p>线性回归模型是一种基本的回归分析方法，用于描述两个或多个变量之间的关系。在线性回归中，目标是通过一个==线性方程==来表示自变量（输入特征）与因变量（输出）之间的关系，从而预测因变量的值。</p>
<p><img src="/2024/08/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image-20240820215458422.png" alt="image-20240820215458422"  data-tag='post-image' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
<p>线性回归模型不一定是一条直线。虽然在简单线性回归中，模型表现为一条直线，但在线性回归中，”线性”指的是参数和特征之间的线性关系，而不一定意味着输出必须是线性的表现。通过多元线性回归和特征变换，线性回归可以适应更复杂的关系，包括曲线和曲面。</p>
<h4 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a><strong>代价函数</strong></h4><p><img src="/2024/08/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image-20240820215127030.png" alt="image-20240820215127030"  data-tag='post-image' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
<p><strong>代价函数（Cost Function）</strong>在线性回归模型中起着至关重要的作用，它==用于衡量模型的预测值与实际值之间的误差==，并指导模型参数（如权重）的优化过程。</p>
<ol>
<li><strong>代价函数的定义</strong></li>
</ol>
<p>在线性回归中，最常用的代价函数是<strong>均方误差（Mean Squared Error, MSE）</strong>，其定义为预测值与实际值之间差异的平方和的平均值。对于一个包含 $m$个样本的数据集，代价函数可以表示为：</p>
<p>$J(w_0, w_1, \dots, w_n) = \frac{1}{2m} \sum_{i=1}^{m} \left( \hat{y}_i - y_i \right)^2$</p>
<p>其中：</p>
<ul>
<li>$J(w_0, w_1, \dots, w_n)$ 是代价函数，表示模型的损失。</li>
<li>$\hat{y}_i$ 是模型的第 $i$个样本的预测值。</li>
<li>$y_i$ 是第 $i$ 个样本的实际值。</li>
<li>$m$是数据样本的数量。</li>
</ul>
<p>平方误差用于惩罚较大的预测误差，确保模型对大误差更敏感。</p>
<ol>
<li><strong>代价函数的作用</strong></li>
</ol>
<p>代价函数在线性回归模型中的作用主要有以下几个方面：</p>
<p>(1) <strong>衡量模型性能</strong></p>
<p>代价函数直接反映了模型的性能。通过计算代价函数的值，可以量化模型在当前参数下的预测误差。代价函数的值越小，说明模型的预测越接近实际值，性能越好。</p>
<p>(2) <strong>指导模型优化</strong></p>
<p>模型训练的目标是最小化代价函数，即通过调整模型参数（如权重 $w_0, w_1, \dots, w_n）$，使代价函数的值尽可能小。通常使用梯度下降算法来优化代价函数。梯度下降的基本思想是通过计算代价函数相对于每个参数的梯度，逐步更新参数，直到找到代价函数的最小值。</p>
<p>梯度下降的参数更新规则如下：</p>
<p>$w_j := w_j - \alpha \frac{\partial J(w_0, w_1, \dots, w_n)}{\partial w_j}$</p>
<p>其中：</p>
<ul>
<li>$w_j$是模型的第 $j$个参数。</li>
<li>$\alpha$是学习率，控制参数更新的步长。</li>
<li>$\frac{\partial J(w_0, w_1, \dots, w_n)}{\partial w_j}$ 是代价函数对参数 $w_j$的偏导数，表示参数 $w_j$ 的梯度。</li>
</ul>
<p>通过不断更新参数，梯度下降算法能够逐渐减少代价函数的值，从而优化模型。</p>
<p><strong>(3) 防止过拟合</strong></p>
<p>在某些情况下，线性回归模型可能会出现过拟合，即模型在训练数据上表现很好，但在新数据上表现较差。为了防止过拟合，代价函数中可以加入正则化项，如L2正则化（岭回归）或L1正则化（Lasso回归），这会在优化过程中对模型参数施加惩罚，使模型更加简单和泛化能力更强。</p>
<p>例如，L2正则化的代价函数形式为：</p>
<p>$J(w_0, w_1, \dots, w_n) = \frac{1}{2m} \sum_{i=1}^{m} \left( \hat{y}_i - y_i \right)^2 + \frac{\lambda}{2} \sum_{j=1}^{n} w_j^2$</p>
<p>其中，$\lambda$是正则化强度的超参数，表示对模型复杂度的惩罚程度。</p>
<p><strong>总结</strong></p>
<p>代价函数在线性回归模型中的作用是通过量化模型的预测误差来衡量模型性能，并指导模型参数的优化过程。通过最小化代价函数，模型能够在训练数据和测试数据上都表现得更好，从而实现有效的预测。</p>
<p><img src="/2024/08/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image-20240820222002950.png" alt="image-20240820222002950"  data-tag='post-image' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
<p><img src="/2024/08/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image-20240820222146415.png" alt="image-20240820222146415"  data-tag='post-image' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
<h4 id="梯度下降和学习率"><a href="#梯度下降和学习率" class="headerlink" title="梯度下降和学习率"></a><strong>梯度下降和学习率</strong></h4><p><strong>梯度下降直观展示：</strong></p>
<p><img src="/2024/08/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image-20240820230755354.png" alt="image-20240820230755354"  data-tag='post-image' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
<p><strong>梯度下降算法：</strong></p>
<p><img src="/2024/08/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image-20240820230714473.png" alt="image-20240820230714473"  data-tag='post-image' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
<p><strong>理解梯度下降：</strong></p>
<p><img src="/2024/08/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image-20240820231215009.png" alt="image-20240820231215009"  data-tag='post-image' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
<p><strong>学习率：</strong></p>
<p><img src="/2024/08/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image-20240820231551920.png" alt="image-20240820231551920"  data-tag='post-image' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
<p>梯度下降（Gradient Descent）是机器学习和优化算法中常用的一种优化方法，主要用于通过最小化代价函数（损失函数）来训练模型，优化模型参数。梯度下降通过反复调整参数，使得代价函数的值逐步减小，最终达到最小值或一个局部最小值。</p>
<ol>
<li><strong>梯度下降的基本思想</strong></li>
</ol>
<p>梯度下降的核心思想是通过计算代价函数对模型参数的梯度（即偏导数），找到代价函数下降最快的方向，并沿着该方向更新模型参数，直到达到收敛。</p>
<p>对于线性回归模型，其代价函数可以表示为：</p>
<p>$J(w_0, w_1, \dots, w_n) = \frac{1}{2m} \sum_{i=1}^{m} \left( \hat{y}_i - y_i \right)^2$</p>
<p>梯度下降算法通过以下步骤来最小化代价函数：</p>
<ol>
<li><strong>初始化参数：</strong> 从某个初始点开始（通常是随机初始化）。</li>
<li><strong>计算梯度：</strong> 计算代价函数相对于每个参数的偏导数，即梯度，表示在当前参数值下，代价函数的变化趋势。</li>
<li><strong>更新参数：</strong> 根据梯度的反方向更新参数，即沿着代价函数下降最快的方向调整参数。</li>
<li><p><strong>重复步骤2和3：</strong> 迭代更新参数，直到达到预设的收敛条件（如梯度接近零，或代价函数的值变化很小，==注意这里有可能是局部最小值而不是全局最小值==）。</p>
</li>
<li><p><strong>梯度下降的参数更新公式</strong></p>
</li>
</ol>
<p>假设代价函数是$J(w_0, w_1, \dots, w_n)$，则对参数 $w_j$的梯度下降更新规则为：</p>
<p>$w_j = w_j - \alpha \frac{\partial J(w_0, w_1, \dots, w_n)}{\partial w_j}$</p>
<p>其中：</p>
<ul>
<li>$w_j$是模型的第 $j$ 个参数。</li>
<li>$\alpha$是学习率（Learning Rate），控制每次更新的步长。</li>
<li>$\frac{\partial J(w_0, w_1, \dots, w_n)}{\partial w_j}$ 是代价函数对参数 $w_j$ 的偏导数，表示参数 $w_j$ 的梯度。</li>
</ul>
<ol>
<li><strong>梯度下降的种类</strong></li>
</ol>
<p>根据每次参数更新时使用的样本数量，梯度下降可以分为三种常见类型：</p>
<ul>
<li><strong>批量梯度下降（Batch Gradient Descent, BGD）：</strong><ul>
<li>使用整个训练集来计算代价函数的梯度，并一次性更新参数。</li>
<li>优点：收敛较稳定。</li>
<li>缺点：在大数据集上效率较低，每次更新都需要遍历整个数据集。</li>
</ul>
</li>
<li><strong>随机梯度下降（Stochastic Gradient Descent, SGD）：</strong><ul>
<li>每次参数更新时只使用一个样本，计算该样本对应的梯度并更新参数。</li>
<li>优点：计算效率高，适合大规模数据集。</li>
<li>缺点：更新过程中波动较大，可能会导致收敛不稳定。</li>
</ul>
</li>
<li><strong>小批量梯度下降（Mini-batch Gradient Descent, MBGD）：</strong><ul>
<li>每次参数更新时使用一小部分样本（称为小批量，mini-batch）来计算梯度（==内部取平均==）。</li>
<li>优点：在计算效率和收敛稳定性之间取得平衡，是实际应用中最常用的方法。</li>
</ul>
</li>
</ul>
<ol>
<li><strong>学习率的重要性</strong></li>
</ol>
<p>学习率 $\alpha$是梯度下降算法中的一个关键超参数，它决定了每次更新时参数的调整幅度。如果学习率过大，参数更新步长过大，可能会导致错过最优解甚至发散；如果学习率过小，参数更新步长过小，优化过程会非常缓慢，甚至可能陷入局部最优解。</p>
<ol>
<li><strong>梯度下降的挑战</strong></li>
</ol>
<p>梯度下降在实际应用中也面临一些挑战：</p>
<ul>
<li><strong>局部最优问题：</strong> 在非凸代价函数中，梯度下降可能会陷入局部最优，而不是全局最优。</li>
<li><strong>学习率调节：</strong> 选择合适的学习率至关重要，需要通过调参或使用自适应学习率算法（如Adam、RMSprop等）来自动调整学习率。</li>
</ul>
<p><strong>总结</strong></p>
<p>梯度下降是用于优化模型参数的一种常用方法，其通过计算代价函数的梯度，并沿着梯度的反方向更新参数，从而逐步最小化代价函数。根据使用样本的数量，梯度下降可分为批量梯度下降、随机梯度下降和小批量梯度下降。学习率的选择是梯度下降算法成功的关键，影响着优化的效率和结果。</p>
<h3 id="week2"><a href="#week2" class="headerlink" title="week2"></a>week2</h3><h4 id="多维特征"><a href="#多维特征" class="headerlink" title="多维特征"></a>多维特征</h4><p>目前为止，我们探讨了单变量/特征的回归模型，现在对房价模型增加更多的特征，例如房间数楼层等，构成一个含有多个变量的模型，模型中的特征为$(x_1,x_2,…,x_n)$。</p>
<p><img src="/2024/08/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image-20240913154459899.png" alt="image-20240913154459899"  data-tag='post-image' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
<p>增添更多特征后，我们引入一系列新的注释：<br>$n$ 代表特征的数量</p>
<p>${\vec{x}}^i$代表第 $i$个训练实例，是特征矩阵中的第行，是一个向量（vector）。</p>
<p>$x^{(i)}_j$代表特征矩阵中第$i$行的第j个特征，也就是第$i$个训练实例的第$j$个特征。</p>
<p><strong>多元线性回归模型定义：</strong></p>
<p><img src="/2024/08/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image-20240913160303376.png" alt="image-20240913160303376"  data-tag='post-image' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
<h4 id="矢量化"><a href="#矢量化" class="headerlink" title="矢量化"></a><strong>矢量化</strong></h4><p>使得代码更短、更容易、阅读运行速度更快。</p>
<p><img src="/2024/08/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image-20240916112559624.png" alt="image-20240916112559624"  data-tag='post-image' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
<h4 id="特征缩放"><a href="#特征缩放" class="headerlink" title="特征缩放"></a>特征缩放</h4><p><strong>特征缩放的定义，原因</strong>  </p>
<p>  特征缩放（feature scaling）是在机器学习中常用的数据预处理步骤，是数据预处理步骤中及其容易被忽略的一步。</p>
<p>  为什么要特征缩放？考虑一下 kNN算法，如果我们有两个特征，特征 A 的取值范围在 [1，10]，特征 B 的取值范围在 [1, 100000]。那么在计算欧式距离时，特征 A 的贡献几乎被特征 B 给覆盖了。</p>
<p>  特征缩放的目的就是使不同特征的数值范围相似，以确保模型的性能和收敛性能良好。绝大多数机器学习算法及优化算法（比如梯度下降）都期望特征取值在同样的范围。</p>
<h3 id="week3"><a href="#week3" class="headerlink" title="week3"></a>week3</h3><h4 id="逻辑回归-sigmoid"><a href="#逻辑回归-sigmoid" class="headerlink" title="逻辑回归(sigmoid)"></a>逻辑回归(sigmoid)</h4><p>作用是输入特征或集合特征X并输出一个介于0和1之间的数字</p>
<p><strong>Sigmoid函数</strong></p>
<p>Sigmoid函数是一种常用的激活函数，特别是在二分类问题中。</p>
<p><strong>激活函数的作用</strong></p>
<p>激活函数在神经网络中的作用可以总结为以下几点：</p>
<ol>
<li><strong>引入非线性</strong>：<ul>
<li>如果没有激活函数，神经网络中的每一层只会执行线性变换，最终整个网络也只是一个线性函数，即使增加层数也无法增加模型的复杂度。</li>
<li>激活函数能够将线性输入转换为非线性输出，使得神经网络能够学习复杂的模式和关系。</li>
</ul>
</li>
<li><strong>控制输出范围</strong>：<ul>
<li>像Sigmoid这样的激活函数将输出限制在一个特定范围内（例如 0 到 1），这对于处理某些任务（如概率估计）非常有帮助。</li>
</ul>
</li>
<li><strong>提升模型的表达能力</strong>：<ul>
<li>不同的激活函数引入了不同的非线性，使得网络能够表达和拟合复杂的函数和特征。例如，ReLU 函数使得网络可以通过不同的神经元来激活不同的特征，学习到丰富的表达形式。</li>
</ul>
</li>
</ol>
<p><img src="/2024/08/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image-20241004154412045.png" alt="image-20241004154412045"  data-tag='post-image' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
<h4 id="决策边界-Decision-Boundary"><a href="#决策边界-Decision-Boundary" class="headerlink" title="决策边界(Decision Boundary)"></a>决策边界(Decision Boundary)</h4><p><strong>决策边界的定义</strong></p>
<p>决策边界是指用于将输入空间分割成不同区域的边界，每个区域对应一个类别。在分类任务中，决策边界将数据划分为不同的类，以便模型能够根据输入的特征预测所属的类别。</p>
<p><strong>决策边界的特性</strong></p>
<ol>
<li>由模型确定：<ul>
<li>决策边界由分类模型自动学习并确定。不同的模型类型产生的决策边界可能形状各异。例如，逻辑回归通常会产生线性决策边界，而神经网络能够学习更加复杂的、非线性的决策边界。</li>
</ul>
</li>
<li>非线性 vs. 线性决策边界：<ul>
<li>线性决策边界：例如逻辑回归，决策边界是一个直线或平面，将不同类别分开。<ul>
<li>公式：对于逻辑回归，决策边界的方程是通过权重和偏置得到的线性表达式 $w^T x + b = 0$。</li>
</ul>
</li>
<li><strong>非线性决策边界</strong>：例如在神经网络或支持向量机中，非线性激活函数可以创建更加复杂的决策边界，适合处理复杂数据。</li>
</ul>
</li>
</ol>
<p><strong>通过激活函数实现的决策边界</strong></p>
<p>以逻辑回归为例，使用 Sigmoid 激活函数来实现分类：</p>
<ul>
<li>Sigmoid 函数输出的值是一个概率，介于 0 到 1 之间。</li>
<li>当概率大于等于某个阈值（例如 0.5）时，样本被归为正类，否则归为负类。</li>
<li>通过 Sigmoid 函数，可以得到决策边界，满足 $w^T x + b = 0$ 的点集即为模型用来区分类别的边界。</li>
</ul>
<p><img src="/2024/08/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image-20241004160426040.png" alt="image-20241004160426040"  data-tag='post-image' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
<h4 id="代价函数与损失函数"><a href="#代价函数与损失函数" class="headerlink" title="代价函数与损失函数"></a>代价函数与损失函数</h4><p><strong>损失函数与代价函数的区别</strong></p>
<p>在机器学习和深度学习中，损失函数和代价函数是两个非常重要的概念，它们用于评估模型预测与真实值之间的差距。二者有相似性，但也有一定的区别：</p>
<ul>
<li><strong>损失函数（Loss Function）</strong>：用于度量单个样本的预测值与真实值之间的误差。</li>
<li><strong>代价函数（Cost Function）</strong>：用于度量整个数据集上的平均误差，即是对所有样本损失函数的求和或平均值。</li>
</ul>
<p>损失函数可以看作是样本级的误差度量，而代价函数则是整个模型在数据集上的总体误差度量。</p>
<h4 id="逻辑回归中的损失函数"><a href="#逻辑回归中的损失函数" class="headerlink" title="逻辑回归中的损失函数"></a><strong>逻辑回归中的损失函数</strong></h4><p>重新定义的单样本损失函数，输入是经过sigmoid函数处理过的f，介于0和1之间</p>
<p>总结：x的预测值f越远离y的真实值，损失值就会越高。意思就是，当x对应的真实标签是1时，算法别强激励不要预测太接近0的东西</p>
<p><img src="/2024/08/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image-20241004165709766.png" alt="image-20241004165709766"  data-tag='post-image' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
<p><img src="/2024/08/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image-20241004170030497.png" alt="image-20241004170030497"  data-tag='post-image' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
<h4 id="逻辑回归中的代价函数"><a href="#逻辑回归中的代价函数" class="headerlink" title="逻辑回归中的代价函数"></a>逻辑回归中的代价函数</h4><p>将上面的所有的单个样本的损失函数求平均，然后就可以得到全局样本的代价函数，然后再通过梯度下降找到代价函数的最小值。</p>
<p><img src="/2024/08/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image-20241004170611107.png" alt="image-20241004170611107"  data-tag='post-image' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
<p><strong>为什么要换损失函数？</strong></p>
<p>因为在添加上sigmoid函数后，在利用原来的均方差求代价不好利用梯度下降找最小值（原来的单个样本的均方误差也可以叫做一种loss函数）</p>
<p><img src="/2024/08/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image-20241004165623551.png" alt="image-20241004165623551"  data-tag='post-image' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
<h4 id="过拟合-overfitting"><a href="#过拟合-overfitting" class="headerlink" title="过拟合(overfitting)"></a>过拟合(overfitting)</h4><p>左（欠拟合，高偏差），中（好的，具有很好的泛化能力），右（过拟合，过度去适应训练集力争与训练数据0误差，高方差）</p>
<p><strong>高方差和高偏差</strong></p>
<p>偏差： 描述的是预测值（估计值）的期望与真实值之间的差距。偏差越大，越偏离真实数据集。</p>
<p>方差： 描述的是预测值的变化范围，离散程度，也就是离其期望值的距离。方差越大，预测结果数据的分布越散。</p>
<p>理解偏差和方差（bias/variance）的两个关键数据是训练集误差（train set error）和验证集误差（dev set error）</p>
<p>通过查看训练集误差和验证集误差，我们便可以诊断算法是否具有高方差</p>
<p><img src="/2024/08/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/964016-20200818164617337-1526902192.png" alt="img"  data-tag='post-image' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
<p><img src="/2024/08/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image-20241004194114573.png" alt="image-20241004194114573"  data-tag='post-image' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
<p><img src="/2024/08/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image-20241004194430461.png" alt="image-20241004194430461"  data-tag='post-image' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
<p><strong>避免过拟合的方法</strong></p>
<ul>
<li>足够的训练集样本</li>
<li>选取更有效的数据特征子集</li>
<li>使用正则化减少参数的大小</li>
</ul>
<h4 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h4><p>在原有的代价函数中加入正则化项，来抑制某些参数值过大，对模型影响过大，并由λ控制正则化对参数的影响</p>
<p><img src="/2024/08/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/image-20241004201249759.png" alt="image-20241004201249759"  data-tag='post-image' loading="lazy" onerror='this.onerror=null;this.src="/img/404.jpg"'></p>
<h2 id="Advanced-Learning-Algorithms"><a href="#Advanced-Learning-Algorithms" class="headerlink" title="Advanced Learning Algorithms"></a>Advanced Learning Algorithms</h2><h3 id="week1-1"><a href="#week1-1" class="headerlink" title="week1"></a>week1</h3><h3 id="week2-1"><a href="#week2-1" class="headerlink" title="week2"></a>week2</h3><h3 id="week3-1"><a href="#week3-1" class="headerlink" title="week3"></a>week3</h3><h3 id="week4"><a href="#week4" class="headerlink" title="week4"></a>week4</h3><h2 id="Unsupervised-learning-recommenders-reinforcement-learning"><a href="#Unsupervised-learning-recommenders-reinforcement-learning" class="headerlink" title="Unsupervised learning recommenders reinforcement learning"></a>Unsupervised learning recommenders reinforcement learning</h2><h3 id="week1-2"><a href="#week1-2" class="headerlink" title="week1"></a>week1</h3><h3 id="week2-2"><a href="#week2-2" class="headerlink" title="week2"></a>week2</h3><h3 id="week3-2"><a href="#week3-2" class="headerlink" title="week3"></a>week3</h3>
</article>
    
    

</div>
<div class="trm-post-next-prev row">
    <div class="col-lg-12">
        <!-- title -->
        <h5 class="trm-title-with-divider">
            其他文章
            <span data-number="02"></span>
        </h5>
    </div>
    
    
</div>

    



                    <div class="trm-divider footer-divider"></div>

                    <!-- footer -->
                    <footer class="trm-footer-card trm-scroll-animation">

    

    

    
        <div class="trm-footer-item">
            <span>
                由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v7.3.0
            </span>
            <span class="footer-separator" data-separator=" | "></span>
            <span> 
                主题 - 
                <a rel="noopener" href='https://github.com/MaLuns/hexo-theme-async' target='_blank'>Async</a>
                v2.2.2
            </span>
        </div>
      

     

     
</footer>
                    <!-- footer end -->

                </div>
            </div>
        </div>
    </div>
</div>
            <!-- body end -->

            

    <div id="post-toc" class="trm-post-toc">
      <div class="trm-post-toc-header">
        目录导航
				<span id="post-toc-top">
					置顶
				</span>
      </div>
      <div class="trm-post-toc-content">
        <ol class="trm-toc"><li class="trm-toc-item trm-toc-level-1" title="吴恩达课程学习记录"><a rel="nofollow" class="trm-toc-link" href="#吴恩达课程学习记录"><span class="trm-toc-number">1.</span> <span class="trm-toc-text">吴恩达课程学习记录</span></a><ol class="trm-toc-child"><li class="trm-toc-item trm-toc-level-2" title="Machine Learning: Regression and Classification"><a rel="nofollow" class="trm-toc-link" href="#Machine-Learning-Regression-and-Classification"><span class="trm-toc-number">1.1.</span> <span class="trm-toc-text">Machine Learning: Regression and Classification</span></a><ol class="trm-toc-child"><li class="trm-toc-item trm-toc-level-3" title="week1"><a rel="nofollow" class="trm-toc-link" href="#week1"><span class="trm-toc-number">1.1.1.</span> <span class="trm-toc-text">week1</span></a><ol class="trm-toc-child"><li class="trm-toc-item trm-toc-level-4" title="机器学习"><a rel="nofollow" class="trm-toc-link" href="#机器学习"><span class="trm-toc-number">1.1.1.1.</span> <span class="trm-toc-text">机器学习</span></a></li><li class="trm-toc-item trm-toc-level-4" title="监督学习（Supervised Learning）"><a rel="nofollow" class="trm-toc-link" href="#监督学习（Supervised-Learning）"><span class="trm-toc-number">1.1.1.2.</span> <span class="trm-toc-text">监督学习（Supervised Learning）</span></a></li><li class="trm-toc-item trm-toc-level-4" title="无监督学习（Unsupervised Learning）"><a rel="nofollow" class="trm-toc-link" href="#无监督学习（Unsupervised-Learning）"><span class="trm-toc-number">1.1.1.3.</span> <span class="trm-toc-text">无监督学习（Unsupervised Learning）</span></a></li><li class="trm-toc-item trm-toc-level-4" title="线性回归模型"><a rel="nofollow" class="trm-toc-link" href="#线性回归模型"><span class="trm-toc-number">1.1.1.4.</span> <span class="trm-toc-text">线性回归模型</span></a></li><li class="trm-toc-item trm-toc-level-4" title="代价函数"><a rel="nofollow" class="trm-toc-link" href="#代价函数"><span class="trm-toc-number">1.1.1.5.</span> <span class="trm-toc-text">代价函数</span></a></li><li class="trm-toc-item trm-toc-level-4" title="梯度下降和学习率"><a rel="nofollow" class="trm-toc-link" href="#梯度下降和学习率"><span class="trm-toc-number">1.1.1.6.</span> <span class="trm-toc-text">梯度下降和学习率</span></a></li></ol></li><li class="trm-toc-item trm-toc-level-3" title="week2"><a rel="nofollow" class="trm-toc-link" href="#week2"><span class="trm-toc-number">1.1.2.</span> <span class="trm-toc-text">week2</span></a><ol class="trm-toc-child"><li class="trm-toc-item trm-toc-level-4" title="多维特征"><a rel="nofollow" class="trm-toc-link" href="#多维特征"><span class="trm-toc-number">1.1.2.1.</span> <span class="trm-toc-text">多维特征</span></a></li><li class="trm-toc-item trm-toc-level-4" title="矢量化"><a rel="nofollow" class="trm-toc-link" href="#矢量化"><span class="trm-toc-number">1.1.2.2.</span> <span class="trm-toc-text">矢量化</span></a></li><li class="trm-toc-item trm-toc-level-4" title="特征缩放"><a rel="nofollow" class="trm-toc-link" href="#特征缩放"><span class="trm-toc-number">1.1.2.3.</span> <span class="trm-toc-text">特征缩放</span></a></li></ol></li><li class="trm-toc-item trm-toc-level-3" title="week3"><a rel="nofollow" class="trm-toc-link" href="#week3"><span class="trm-toc-number">1.1.3.</span> <span class="trm-toc-text">week3</span></a><ol class="trm-toc-child"><li class="trm-toc-item trm-toc-level-4" title="逻辑回归(sigmoid)"><a rel="nofollow" class="trm-toc-link" href="#逻辑回归-sigmoid"><span class="trm-toc-number">1.1.3.1.</span> <span class="trm-toc-text">逻辑回归(sigmoid)</span></a></li><li class="trm-toc-item trm-toc-level-4" title="决策边界(Decision Boundary)"><a rel="nofollow" class="trm-toc-link" href="#决策边界-Decision-Boundary"><span class="trm-toc-number">1.1.3.2.</span> <span class="trm-toc-text">决策边界(Decision Boundary)</span></a></li><li class="trm-toc-item trm-toc-level-4" title="代价函数与损失函数"><a rel="nofollow" class="trm-toc-link" href="#代价函数与损失函数"><span class="trm-toc-number">1.1.3.3.</span> <span class="trm-toc-text">代价函数与损失函数</span></a></li><li class="trm-toc-item trm-toc-level-4" title="逻辑回归中的损失函数"><a rel="nofollow" class="trm-toc-link" href="#逻辑回归中的损失函数"><span class="trm-toc-number">1.1.3.4.</span> <span class="trm-toc-text">逻辑回归中的损失函数</span></a></li><li class="trm-toc-item trm-toc-level-4" title="逻辑回归中的代价函数"><a rel="nofollow" class="trm-toc-link" href="#逻辑回归中的代价函数"><span class="trm-toc-number">1.1.3.5.</span> <span class="trm-toc-text">逻辑回归中的代价函数</span></a></li><li class="trm-toc-item trm-toc-level-4" title="过拟合(overfitting)"><a rel="nofollow" class="trm-toc-link" href="#过拟合-overfitting"><span class="trm-toc-number">1.1.3.6.</span> <span class="trm-toc-text">过拟合(overfitting)</span></a></li><li class="trm-toc-item trm-toc-level-4" title="正则化"><a rel="nofollow" class="trm-toc-link" href="#正则化"><span class="trm-toc-number">1.1.3.7.</span> <span class="trm-toc-text">正则化</span></a></li></ol></li></ol></li><li class="trm-toc-item trm-toc-level-2" title="Advanced Learning Algorithms"><a rel="nofollow" class="trm-toc-link" href="#Advanced-Learning-Algorithms"><span class="trm-toc-number">1.2.</span> <span class="trm-toc-text">Advanced Learning Algorithms</span></a><ol class="trm-toc-child"><li class="trm-toc-item trm-toc-level-3" title="week1"><a rel="nofollow" class="trm-toc-link" href="#week1-1"><span class="trm-toc-number">1.2.1.</span> <span class="trm-toc-text">week1</span></a></li><li class="trm-toc-item trm-toc-level-3" title="week2"><a rel="nofollow" class="trm-toc-link" href="#week2-1"><span class="trm-toc-number">1.2.2.</span> <span class="trm-toc-text">week2</span></a></li><li class="trm-toc-item trm-toc-level-3" title="week3"><a rel="nofollow" class="trm-toc-link" href="#week3-1"><span class="trm-toc-number">1.2.3.</span> <span class="trm-toc-text">week3</span></a></li><li class="trm-toc-item trm-toc-level-3" title="week4"><a rel="nofollow" class="trm-toc-link" href="#week4"><span class="trm-toc-number">1.2.4.</span> <span class="trm-toc-text">week4</span></a></li></ol></li><li class="trm-toc-item trm-toc-level-2" title="Unsupervised learning recommenders reinforcement learning"><a rel="nofollow" class="trm-toc-link" href="#Unsupervised-learning-recommenders-reinforcement-learning"><span class="trm-toc-number">1.3.</span> <span class="trm-toc-text">Unsupervised learning recommenders reinforcement learning</span></a><ol class="trm-toc-child"><li class="trm-toc-item trm-toc-level-3" title="week1"><a rel="nofollow" class="trm-toc-link" href="#week1-2"><span class="trm-toc-number">1.3.1.</span> <span class="trm-toc-text">week1</span></a></li><li class="trm-toc-item trm-toc-level-3" title="week2"><a rel="nofollow" class="trm-toc-link" href="#week2-2"><span class="trm-toc-number">1.3.2.</span> <span class="trm-toc-text">week2</span></a></li><li class="trm-toc-item trm-toc-level-3" title="week3"><a rel="nofollow" class="trm-toc-link" href="#week3-2"><span class="trm-toc-number">1.3.3.</span> <span class="trm-toc-text">week3</span></a></li></ol></li></ol></li></ol>
      </div>
    </div>

            
<div class="trm-fixed-container">
    
        <div class="trm-fixed-btn post-toc-btn" data-title="目录">
            <i class="iconfont fas fa-th-list"></i>
        </div>
    
    
        <div class="trm-fixed-btn" data-title="阅读模式" onclick="asyncFun.switchReadMode()">
            <i class="iconfont fas fa-book-reader"></i>
        </div>
    
    
    <div id="trm-back-top" class="trm-fixed-btn" data-title="回到顶部">
        <i class="iconfont fas fa-arrow-up"></i>
    </div>
</div>
        </div>
      </div>
      <!-- scroll container end -->
  </div>
  <!-- app wrapper end -->

  
  <!-- Plugin -->




    
    
<script src="https://unpkg.com/@fancyapps/ui@4.0/dist/fancybox.umd.js"></script>

    

    

    

    <!-- 数学公式 -->
    
        
<script src="https://unpkg.com/katex@latest/dist/katex.min.js" data-swup-reload-script></script>

        
            
<script src="https://unpkg.com/katex@latest/dist/contrib/copy-tex.min.js" data-swup-reload-script></script>

        
        
<script src="https://unpkg.com/katex@latest/dist/contrib/auto-render.min.js" data-swup-reload-script></script>

        <script data-swup-reload-script>
              window.renderMathInElement(document.body, {
                delimiters: [
                    { left: '$$', right: '$$', display: true },
                    { left: '$', right: '$', display: false },
                    { left: '\\(', right: '\\)', display: false },
                    { left: '\\[', right: '\\]', display: true },
                ],
                ...{},
            })
        </script>
    

    <!-- 评论插件 -->
    
        

        
    

		




    <!-- Service Worker -->
    
    <!-- baidu push -->
    


<script id="async-script" src="/js/main.js?v=2.2.2"></script>

<!-- CDN -->


    

    

    



</body>

</html>